# app.py

import sys
import os
import numpy as np
import pandas as pd
import math
from datetime import datetime
import multiprocessing
from itertools import combinations
from math import comb
from sklearn.decomposition import PCA
from sklearn.utils.class_weight import compute_class_weight
from sklearn.preprocessing import StandardScaler, FunctionTransformer
from sklearn.model_selection import train_test_split, GridSearchCV, TimeSeriesSplit
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.feature_selection import RFE, RFECV, VarianceThreshold
from sklearn.metrics import (
    classification_report, confusion_matrix, roc_auc_score, precision_score, 
    recall_score, average_precision_score, matthews_corrcoef, f1_score
)
from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE
from imblearn.combine import SMOTEENN, SMOTETomek

# ============== PyTorch & Skorch ç›¸å…³ ==============
import torch
import torch.nn as nn
import torch.nn.functional as F
from skorch import NeuralNetClassifier
from skorch.callbacks import EarlyStopping

# ============== Plotly ç›¸å…³ ==============
import plotly.graph_objs as go
from plotly.subplots import make_subplots

# ============== ä»å¤–éƒ¨ function.py å¯¼å…¥å…¬ç”¨å‡½æ•°å’ŒæŒ‡æ ‡å‡½æ•° ==============
from function import *

# ============== å¼•å…¥ Streamlit è¿›è¡ŒWebå±•ç¤º ==============
import streamlit as st
from joblib import Parallel, delayed

# ============== å¯¼å…¥è‡ªå®šä¹‰æ¨¡å‹ç±» ==============
from models import (
    WeightedBCEWithLogitsLoss,
    WeightedCrossEntropyLoss,
    TransformerClassifier,
    MLPClassifierModule
)

# ============== AgGrid ç›¸å…³ ==============
from st_aggrid import AgGrid

# ============== å…¶ä»–å¿…è¦å¯¼å…¥ ==============
import base64
import time

# ============== å¯¼å…¥ Tushare å¹¶è®¾ç½® Token ==============
import tushare as ts

# è®¾ç½®æ‚¨çš„ Tushare Token
ts.set_token('c5c5700a6f4678a1837ad234f2e9ea2a573a26b914b47fa2dbb38aff')
pro = ts.pro_api()

##############################################################################
#                            è®¾ç½®éšæœºç§å­
##############################################################################
def set_seed(seed=42):
    import random
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    # å¯¹äºæŸäº›æ“ä½œï¼Œå¯èƒ½éœ€è¦è®¾ç½®ä»¥ä¸‹å‚æ•°ä»¥ç¡®ä¿å®Œå…¨çš„ç¡®å®šæ€§
    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':16:8'
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    torch.use_deterministic_algorithms(True)

set_seed(42)

# ---------- æ•°æ®è¯»å–ä¸å¤„ç†å‡½æ•° ----------

def read_day_from_tushare(symbol_code, symbol_type='stock'):
    """
    ä½¿ç”¨ Tushare API è·å–è‚¡ç¥¨æˆ–æŒ‡æ•°çš„å…¨éƒ¨æ—¥çº¿è¡Œæƒ…æ•°æ®ã€‚
    å‚æ•°:
    - symbol_code: è‚¡ç¥¨æˆ–æŒ‡æ•°ä»£ç  (å¦‚ "000001.SZ" æˆ– "000300.SH")
    - symbol_type: 'stock' æˆ– 'index' (ä¸åŒºåˆ†å¤§å°å†™)
    è¿”å›:
    - åŒ…å«æ—¥æœŸã€å¼€é«˜ä½æ”¶ã€æˆäº¤é‡ç­‰åˆ—çš„DataFrame
    """
    symbol_type = symbol_type.lower()
    print(f"ä¼ é€’ç»™ read_day_from_tushare çš„ symbol_type: {symbol_type} (ç±»å‹: {type(symbol_type)})")  # è°ƒè¯•è¾“å‡º
    print(f"å°è¯•é€šè¿‡ Tushare è·å–{symbol_type}æ•°æ®: {symbol_code}")
    
    # æ·»åŠ æ–­è¨€ï¼Œç¡®ä¿ symbol_type æ˜¯ 'stock' æˆ– 'index'
    assert symbol_type in ['stock', 'index'], "symbol_type å¿…é¡»æ˜¯ 'stock' æˆ– 'index'"
    
    try:
        if symbol_type == 'stock':
            # è·å–è‚¡ç¥¨æ—¥çº¿æ•°æ®
            df = pro.daily(ts_code=symbol_code, start_date='20000101', end_date='20251231')
            if df.empty:
                print("Tushare è¿”å›çš„è‚¡ç¥¨æ•°æ®ä¸ºç©ºã€‚")
                return pd.DataFrame()
            
            # è½¬æ¢æ—¥æœŸæ ¼å¼å¹¶æ’åº
            df['date'] = pd.to_datetime(df['trade_date'], format='%Y%m%d')
            df = df.sort_values('date')
            
            # é‡å‘½åå’Œé€‰æ‹©éœ€è¦çš„åˆ—
            df = df.rename(columns={
                'open': 'Open',
                'high': 'High',
                'low': 'Low',
                'close': 'Close',
                'vol': 'Volume',
                'amount': 'Amount',
                'trade_date': 'TradeDate'
            })
            df.set_index('date', inplace=True)
            
            # é€‰æ‹©éœ€è¦çš„åˆ—
            required_columns = ['Open', 'High', 'Low', 'Close', 'Volume', 'Amount', 'TradeDate']
            available_columns = [col for col in required_columns if col in df.columns]
            df = df[available_columns]
        
        elif symbol_type == 'index':
            # è·å–æŒ‡æ•°æ—¥çº¿æ•°æ®ï¼Œä½¿ç”¨ index_daily æ¥å£
            df = pro.index_daily(ts_code=symbol_code, start_date='20000101', end_date='20251231')
            if df.empty:
                print("Tushare è¿”å›çš„æŒ‡æ•°æ•°æ®ä¸ºç©ºã€‚")
                return pd.DataFrame()
            
            # è½¬æ¢æ—¥æœŸæ ¼å¼å¹¶æ’åº
            df['date'] = pd.to_datetime(df['trade_date'], format='%Y%m%d')
            df = df.sort_values('date')
            
            # é‡å‘½åå’Œé€‰æ‹©éœ€è¦çš„åˆ—
            df = df.rename(columns={
                'open': 'Open',
                'high': 'High',
                'low': 'Low',
                'close': 'Close',
                'vol': 'Volume',
                'amount': 'Amount',
                'trade_date': 'TradeDate'
            })
            df.set_index('date', inplace=True)
            
            # é€‰æ‹©éœ€è¦çš„åˆ—ï¼Œå¤„ç†å¯èƒ½ç¼ºå¤±çš„å­—æ®µ
            required_columns = ['Open', 'High', 'Low', 'Close', 'Volume', 'Amount', 'TradeDate']
            available_columns = [col for col in required_columns if col in df.columns]
            df = df[available_columns]
        
        print(f"é€šè¿‡ Tushare è·å–äº† {len(df)} æ¡è®°å½•ã€‚")
        print(f"æ•°æ®æ¡†çš„åˆ—ï¼š{df.columns.tolist()}")
        print(f"æ•°æ®æ¡†å‰5è¡Œï¼š\n{df.head()}")
        return df
    except AssertionError as ae:
        print(f"æ–­è¨€é”™è¯¯ï¼š{ae}")
        return pd.DataFrame()
    except Exception as e:
        print(f"é€šè¿‡ Tushare è·å–æ•°æ®å¤±è´¥ï¼š{e}")
        return pd.DataFrame()


def select_time(df, start_time='20230101', end_time='20240910'):
    """
    æ ¹æ®æŒ‡å®šçš„æ—¶é—´èŒƒå›´ç­›é€‰æ•°æ®ã€‚
    å‚æ•°:
    - df: åŒ…å«æ—¥æœŸç´¢å¼•çš„DataFrame
    - start_time: èµ·å§‹æ—¶é—´ (å­—ç¬¦ä¸², æ ¼å¼ 'YYYYMMDD')
    - end_time: æˆªæ­¢æ—¶é—´ (å­—ç¬¦ä¸², æ ¼å¼ 'YYYYMMDD')
    è¿”å›:
    - ç­›é€‰åçš„DataFrame
    """
    print(f"ç­›é€‰æ—¥æœŸèŒƒå›´: {start_time} è‡³ {end_time}")
    try:
        start_time = pd.to_datetime(start_time, format='%Y%m%d')
        end_time = pd.to_datetime(end_time, format='%Y%m%d')
    except Exception as e:
        print(f"æ—¥æœŸè½¬æ¢é”™è¯¯ï¼š{e}")
        return pd.DataFrame()
    df_filtered = df.loc[start_time:end_time]
    print(f"ç­›é€‰åæ•°æ®é•¿åº¦: {len(df_filtered)}")
    return df_filtered


##############################################################################
#                       å®šä¹‰å…¨å±€èº«ä»½å‡½æ•°æ›¿ä»£ lambda
##############################################################################
def identity(x):
    return x

##############################################################################
#                       åˆ†ç±»å™¨å·¥å‚å‡½æ•°
##############################################################################
def get_class_weights(y):
    classes = np.unique(y)
    weights = compute_class_weight(class_weight='balanced', classes=classes, y=y)
    return torch.tensor(weights, dtype=torch.float32)

def get_transformer_classifier(num_features, window_size, class_weights=None):
    if class_weights is not None:
        loss = WeightedCrossEntropyLoss(weight=class_weights)
    else:
        loss = nn.CrossEntropyLoss()
    
    net = NeuralNetClassifier(
        module=TransformerClassifier,
        module__num_features=num_features,
        module__window_size=window_size,
        module__hidden_dim=64,
        module__nhead=8,
        module__num_encoder_layers=2,
        module__dropout=0.1,
        
        max_epochs=10,
        lr=1e-3,
        optimizer=torch.optim.Adam,
        criterion=loss,
        batch_size=64,
        train_split=None,
        verbose=0,
        device='cuda' if torch.cuda.is_available() else 'cpu'
    )
    return ('transformer', net)

def get_mlp_classifier(input_dim, class_weights=None):
    if class_weights is not None:
        if isinstance(class_weights, torch.Tensor):
            class_weights = class_weights.float()
        loss = nn.CrossEntropyLoss(weight=class_weights)
    else:
        loss = nn.CrossEntropyLoss()
    
    net = NeuralNetClassifier(
        module=MLPClassifierModule,
        module__input_dim=input_dim,
        module__hidden_dim=64,
        module__output_dim=2,
        module__dropout=0.5,
        
        criterion=loss,
        optimizer=torch.optim.Adam,
        max_epochs=50,
        lr=1e-3,
        batch_size=64,
        train_split=None,
        verbose=0,
        device='cuda' if torch.cuda.is_available() else 'cpu'
    )
    return ('mlp', net)

def get_classifier(classifier_name, num_features=None, window_size=10, class_weight=None):
    if classifier_name == 'éšæœºæ£®æ—':
        return ('rf', RandomForestClassifier(
            n_estimators=200,
            max_features='sqrt',
            random_state=42,
            n_jobs=-1,
            class_weight=class_weight
        ))
    
    elif classifier_name == 'æ”¯æŒå‘é‡æœº':
        return ('svc', SVC(
            probability=True,
            kernel='linear',
            random_state=42,
            class_weight=class_weight
        ))
    
    elif classifier_name == 'é€»è¾‘å›å½’':
        return ('lr', LogisticRegression(
            max_iter=1000,
            random_state=42,
            class_weight=class_weight
        ))
    
    elif classifier_name == 'æ¢¯åº¦æå‡':
        return ('gb', GradientBoostingClassifier(
            random_state=42
        ))
    
    elif classifier_name == 'Transformer':
        if num_features is None:
            raise ValueError("num_featureså¿…é¡»ä¸ºTransformeræ¨¡å‹æŒ‡å®š")
        return get_transformer_classifier(num_features, window_size, class_weights=class_weight)
    
    elif classifier_name == 'æ·±åº¦å­¦ä¹ ':
        if num_features is None:
            raise ValueError("num_featureså¿…é¡»ä¸ºMLPæ¨¡å‹æŒ‡å®š")
        return get_mlp_classifier(num_features, class_weights=class_weight)
    
    else:
        raise ValueError(f"æœªçŸ¥çš„åˆ†ç±»å™¨åç§°: {classifier_name}")

##############################################################################
#                       æ•°æ®é¢„å¤„ç†å‡½æ•°
##############################################################################
@st.cache_data
def preprocess_data(data, N, mixture_depth, mark_labels=True, min_features_to_select=10, max_features_for_mixture=50):
    """
    å®Œæ•´ä¿ç•™æ‚¨åŸæœ¬çš„ preprocess_data é€»è¾‘ï¼Œä¸å†çœç•¥ã€‚
    """
    print("å¼€å§‹é¢„å¤„ç†æ•°æ®...")
    data = data.sort_values('TradeDate').copy()
    data.index = pd.to_datetime(data['TradeDate'], format='%Y%m%d')

    # ----------------------------------------
    # 1) å„ç±»æŠ€æœ¯æŒ‡æ ‡
    # ----------------------------------------
    data['MA_5'] = data['Close'].rolling(window=5).mean()
    data['MA_20'] = data['Close'].rolling(window=20).mean()
    data['MA_50'] = data['Close'].rolling(window=50).mean()
    data['MA_200'] = data['Close'].rolling(window=200).mean()
    data['EMA_5'] = data['Close'].ewm(span=5, adjust=False).mean()
    data['EMA_20'] = data['Close'].ewm(span=20, adjust=False).mean()
    data['Price_MA20_Diff'] = (data['Close'] - data['MA_20']) / data['MA_20']
    data['MA5_MA20_Cross'] = np.where(data['MA_5'] > data['MA_20'], 1, 0)
    data['MA5_MA20_Cross_Diff'] = data['MA5_MA20_Cross'].diff()
    data['Slope_MA5'] = data['MA_5'].diff()

    data['RSI_14'] = compute_RSI(data['Close'], period=14)
    data['MACD'], data['MACD_signal'] = compute_MACD(data['Close'])
    data['MACD_Cross'] = np.where(data['MACD'] > data['MACD_signal'], 1, 0)
    data['MACD_Cross_Diff'] = data['MACD_Cross'].diff()
    data['K'], data['D'] = compute_KD(data['High'], data['Low'], data['Close'], period=14)
    data['Momentum_10'] = compute_momentum(data['Close'], period=10)
    data['ROC_10'] = compute_ROC(data['Close'], period=10)
    data['RSI_Reversal'] = (data['RSI_14'] > 70).astype(int) - (data['RSI_14'] < 30).astype(int)
    data['Reversal_Signal'] = (
        (data['Close'] > data['High'].rolling(window=10).max()).astype(int)
        - (data['Close'] < data['Low'].rolling(window=10).min()).astype(int)
    )

    data['UpperBand'], data['MiddleBand'], data['LowerBand'] = compute_Bollinger_Bands(data['Close'], period=20)
    data['ATR_14'] = compute_ATR(data['High'], data['Low'], data['Close'], period=14)
    data['Volatility_10'] = compute_volatility(data['Close'], period=10)
    data['Bollinger_Width'] = (data['UpperBand'] - data['LowerBand']) / data['MiddleBand']

    if 'Volume' in data.columns:
        data['OBV'] = compute_OBV(data['Close'], data['Volume'])
        data['Volume_Change'] = data['Volume'].pct_change()
        data['VWAP'] = compute_VWAP(data['High'], data['Low'], data['Close'], data['Volume'])
        data['MFI_14'] = compute_MFI(data['High'], data['Low'], data['Close'], data['Volume'], period=14)
        data['CMF_20'] = compute_CMF(data['High'], data['Low'], data['Close'], data['Volume'], period=20)
        data['Chaikin_Osc'] = compute_chaikin_oscillator(data['High'], data['Low'], data['Close'], data['Volume'], short_period=3, long_period=10)
    else:
        data['OBV'] = np.nan
        data['Volume_Change'] = np.nan
        data['VWAP'] = np.nan
        data['MFI_14'] = np.nan
        data['CMF_20'] = np.nan
        data['Chaikin_Osc'] = np.nan

    data['CCI_20'] = compute_CCI(data['High'], data['Low'], data['Close'], period=20)
    data['Williams_%R_14'] = compute_williams_r(data['High'], data['Low'], data['Close'], period=14)
    data['ZScore_20'] = compute_zscore(data['Close'], period=20)
    data['Price_Mean_Diff'] = (data['Close'] - data['Close'].rolling(window=10).mean()) / data['Close'].rolling(window=10).mean()
    data['High_Mean_Diff'] = (data['High'] - data['High'].rolling(window=10).mean()) / data['High'].rolling(window=10).mean()
    data['Low_Mean_Diff'] = (data['Low'] - data['Low'].rolling(window=10).mean()) / data['Low'].rolling(window=10).mean()

    data['Plus_DI'], data['Minus_DI'], data['ADX_14'] = compute_ADX(data['High'], data['Low'], data['Close'], period=14)

    data['TRIX_15'] = compute_TRIX(data['Close'], period=15)
    data['Ultimate_Osc'] = compute_ultimate_oscillator(data['High'], data['Low'], data['Close'], short_period=7, medium_period=14, long_period=28)
    data['PPO'] = compute_PPO(data['Close'], fast_period=12, slow_period=26)
    data['DPO_20'] = compute_DPO(data['Close'], period=20)
    data['KST'], data['KST_signal'] = compute_KST(data['Close'], r1=10, r2=15, r3=20, r4=30, sma1=10, sma2=10, sma3=10, sma4=15)
    data['KAMA_10'] = compute_KAMA(data['Close'], n=10, pow1=2, pow2=30)

    data['Seasonality'] = np.sin(2 * np.pi * data.index.dayofyear / 365)
    data['one'] = 1

    if mark_labels:
        print("å¯»æ‰¾å±€éƒ¨é«˜ç‚¹å’Œä½ç‚¹(ä»…è®­ç»ƒé˜¶æ®µ)...")
        N = int(N)
        data = identify_low_troughs(data, N)
        data = identify_high_peaks(data, N)
    else:
        # éªŒè¯æ—¶ä¸æ ‡æ³¨ï¼Œä½†éœ€è¦å ä½åˆ—
        if 'Peak' in data.columns:
            data.drop(columns=['Peak'], inplace=True)
        if 'Trough' in data.columns:
            data.drop(columns=['Trough'], inplace=True)
        data['Peak'] = 0
        data['Trough'] = 0

    # æ·»åŠ è®¡æ•°ç‰¹å¾
    print("æ·»åŠ è®¡æ•°æŒ‡æ ‡...")
    data['PriceChange'] = data['Close'].diff()
    data['Up'] = np.where(data['PriceChange'] > 0, 1, 0)
    data['Down'] = np.where(data['PriceChange'] < 0, 1, 0)
    data['ConsecutiveUp'] = data['Up'] * (
        data['Up'].groupby((data['Up'] != data['Up'].shift()).cumsum()).cumcount() + 1
    )
    data['ConsecutiveDown'] = data['Down'] * (
        data['Down'].groupby((data['Down'] != data['Down'].shift()).cumsum()).cumcount() + 1
    )

    window_size = 10
    data['Cross_MA5'] = np.where(data['Close'] > data['MA_5'], 1, 0)
    data['Cross_MA5_Count'] = data['Cross_MA5'].rolling(window=window_size).sum()

    if 'Volume' in data.columns:
        data['Volume_MA_5'] = data['Volume'].rolling(window=5).mean()
        data['Volume_Spike'] = np.where(data['Volume'] > data['Volume_MA_5'] * 1.5, 1, 0)
        data['Volume_Spike_Count'] = data['Volume_Spike'].rolling(window=10).sum()
    else:
        data['Volume_Spike_Count'] = np.nan

    # æ„å»ºåŸºç¡€å› å­
    print("æ„å»ºåŸºç¡€å› å­...")
    data['Close_MA5_Diff'] = data['Close'] - data['MA_5']
    data['Pch'] = data['Close'] / data['Close'].shift(1) - 1
    data['MA5_MA20_Diff'] = data['MA_5'] - data['MA_20']
    data['RSI_Signal'] = data['RSI_14'] - 50
    data['MACD_Diff'] = data['MACD'] - data['MACD_signal']
    band_range = (data['UpperBand'] - data['LowerBand']).replace(0, np.nan)
    data['Bollinger_Position'] = (data['Close'] - data['MiddleBand']) / band_range
    data['Bollinger_Position'] = data['Bollinger_Position'].fillna(0)
    data['K_D_Diff'] = data['K'] - data['D']

    base_features = [
        'Close_MA5_Diff', 'MA5_MA20_Diff', 'RSI_Signal', 'MACD_Diff',
        'Bollinger_Position', 'K_D_Diff', 'ConsecutiveUp', 'ConsecutiveDown',
        'Cross_MA5_Count', 'Volume_Spike_Count','one','Close','Pch','CCI_20'
    ]

    base_features.extend([
        'Williams_%R_14', 'OBV', 'VWAP','ZScore_20', 'Plus_DI', 'Minus_DI',
        'ADX_14','Bollinger_Width', 'Slope_MA5', 'Volume_Change', 
        'Price_Mean_Diff','High_Mean_Diff','Low_Mean_Diff','MA_5','MA_20','MA_50',
        'MA_200','EMA_5','EMA_20'
    ])

    base_features.extend([
        'MFI_14','CMF_20','TRIX_15','Ultimate_Osc','Chaikin_Osc','PPO',
        'DPO_20','KST','KST_signal','KAMA_10'
    ])

    if 'Volume' in data.columns:
        base_features.append('Volume')

    print("å¯¹åŸºç¡€ç‰¹å¾è¿›è¡Œæ–¹å·®è¿‡æ»¤...")
    X_base = data[base_features].fillna(0)
    selector = VarianceThreshold(threshold=0.0001)
    selector.fit(X_base)
    filtered_features = [f for f, s in zip(base_features, selector.get_support()) if s]
    print(f"æ–¹å·®è¿‡æ»¤åå‰©ä½™ç‰¹å¾æ•°ï¼š{len(filtered_features)}ï¼ˆä»{len(base_features)}å‡å°‘ï¼‰")
    base_features = filtered_features

    print("å¯¹åŸºç¡€ç‰¹å¾è¿›è¡Œç›¸å…³æ€§è¿‡æ»¤...")
    corr_matrix = data[base_features].corr().abs()
    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
    to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]
    base_features = [f for f in base_features if f not in to_drop]
    print(f"ç›¸å…³æ€§è¿‡æ»¤åå‰©ä½™ç‰¹å¾æ•°ï¼š{len(base_features)}")

    # ç”Ÿæˆæ··åˆå› å­
    print(f"ç”Ÿæˆæ··åˆå› å­ï¼Œæ··åˆæ·±åº¦ä¸º {mixture_depth}...")
    if mixture_depth > 1:
        operators = ['+', '-', '*', '/']
        mixed_features = base_features.copy()
        current_depth_features = base_features.copy()

        for depth in range(2, mixture_depth + 1):
            print(f"ç”Ÿæˆæ·±åº¦ {depth} çš„æ··åˆå› å­...")
            new_features = []
            feature_pairs = combinations(current_depth_features, 2)

            for f1, f2 in feature_pairs:
                for op in operators:
                    new_feature_name = f'({f1}){op}({f2})_d{depth}'
                    try:
                        if op == '+':
                            data[new_feature_name] = data[f1] + data[f2]
                        elif op == '-':
                            data[new_feature_name] = data[f1] - data[f2]
                        elif op == '*':
                            data[new_feature_name] = data[f1] * data[f2]
                        elif op == '/':
                            denom = data[f2].replace(0, np.nan)
                            data[new_feature_name] = data[f1] / denom

                        data[new_feature_name] = data[new_feature_name].replace([np.inf, -np.inf], np.nan).fillna(0)
                        new_features.append(new_feature_name)
                    except Exception as e:
                        print(f"æ— æ³•è®¡ç®—ç‰¹å¾ {new_feature_name}ï¼Œé”™è¯¯ï¼š{e}")

            # å¯¹æœ¬è½®æ–°ç‰¹å¾è¿›è¡Œæ–¹å·®å’Œç›¸å…³æ€§è¿‡æ»¤
            if new_features:
                X_new = data[new_features].fillna(0)
                selector = VarianceThreshold(threshold=0.0001)
                selector.fit(X_new)
                new_features = [nf for nf, s in zip(new_features, selector.get_support()) if s]

                if len(new_features) > 1:
                    corr_matrix_new = data[new_features].corr().abs()
                    upper_new = corr_matrix_new.where(np.triu(np.ones(corr_matrix_new.shape), k=1).astype(bool))
                    to_drop_new = [column for column in upper_new.columns if any(upper_new[column] > 0.95)]
                    new_features = [f for f in new_features if f not in to_drop_new]

            mixed_features.extend(new_features)
            current_depth_features = new_features.copy()

        all_features = mixed_features.copy()

        # ä½¿ç”¨ PCA é™ç»´
        print("è¿›è¡Œ PCA é™ç»´...")
        pca_components = min(100, len(all_features))
        pca = PCA(n_components=pca_components)
        X_mixed = data[all_features].fillna(0).values
        X_mixed_pca = pca.fit_transform(X_mixed)
        pca_feature_names = [f'PCA_{i}' for i in range(pca_components)]
        for i in range(pca_components):
            data[pca_feature_names[i]] = X_mixed_pca[:, i]
        all_features = pca_feature_names
    else:
        all_features = base_features.copy()

    print(f"æœ€ç»ˆç‰¹å¾æ•°é‡ï¼š{len(all_features)}")

    # åˆ é™¤ç¼ºå¤±å€¼
    required_cols = [
        'Close_MA5_Diff', 'MA5_MA20_Diff', 'RSI_Signal', 'MACD_Diff',
        'Bollinger_Position', 'K_D_Diff'
    ]
    for col in required_cols:
        if col not in data.columns:
            raise ValueError(f"åˆ— {col} æœªè¢«åˆ›å»ºï¼Œè¯·æ£€æŸ¥æ•°æ®å’Œè®¡ç®—æ­¥éª¤ã€‚")

    print("åˆ é™¤ç¼ºå¤±å€¼...")
    initial_length = len(data)
    data = data.dropna().copy()
    final_length = len(data)
    print(f"æ•°æ®é¢„å¤„ç†å‰é•¿åº¦: {initial_length}, æ•°æ®é¢„å¤„ç†åé•¿åº¦: {final_length}")

    return data, all_features

##############################################################################
#                       ä¼˜åŒ–é˜ˆå€¼å‡½æ•°
##############################################################################
@st.cache_data
def optimize_threshold(y_true, y_proba):
    """
    ä¼˜åŒ–åˆ†ç±»é˜ˆå€¼ä»¥æœ€å¤§åŒ– F1 åˆ†æ•°ã€‚

    å‚æ•°:
    - y_true: çœŸå®æ ‡ç­¾
    - y_proba: é¢„æµ‹æ¦‚ç‡

    è¿”å›:
    - æœ€ä½³é˜ˆå€¼
    """
    best_thresh = 0.5
    best_f1 = -1

    for thresh in np.linspace(0, 1, 101):
        y_pred_temp = (y_proba > thresh).astype(int)
        score = f1_score(y_true, y_pred_temp)
        if score > best_f1:
            best_f1 = score
            best_thresh = thresh

    return best_thresh

##############################################################################
#                       åˆ›å»ºåºåˆ—æ•°æ®å‡½æ•°
##############################################################################
@st.cache_data
def create_sequences(X, y=None, window_size=10):
    """
    åˆ›å»ºæ—¶é—´åºåˆ—æ•°æ®çš„å‡½æ•°ã€‚

    å‚æ•°:
    - X: ç‰¹å¾æ•°ç»„
    - y: æ ‡ç­¾æ•°ç»„ï¼ˆå¯é€‰ï¼‰
    - window_size: çª—å£å¤§å°

    è¿”å›:
    - sequences: åºåˆ—æ•°æ®
    - labels: æ ‡ç­¾ï¼ˆå¦‚æœæä¾›äº† yï¼‰
    """
    sequences = []
    if y is not None:
        labels = []
    
    for i in range(window_size, len(X) + 1):
        seq_x = X[i - window_size:i].astype(np.float32)
        sequences.append(seq_x)
        
        if y is not None:
            seq_y = y[i - 1]
            labels.append(seq_y)
    
    sequences = np.array(sequences)
    
    if y is not None:
        labels = np.array(labels, dtype=np.int64)
        return sequences, labels
    return sequences

##############################################################################
#                       è®­ç»ƒæ¨¡å‹å‡½æ•°
##############################################################################
def train_model_for_label(
    df, N, label_column, all_features, classifier_name, 
    n_features_selected, window_size=10, oversample_method='SMOTE', class_weight=None
):
    print(f"å¼€å§‹è®­ç»ƒ {label_column} æ¨¡å‹...")
    data = df.copy()
    
    # ç‰¹å¾ç›¸å…³æ€§è¿‡æ»¤ï¼ˆå¯é€‰ï¼Œæ ¹æ®éœ€è¦ï¼‰
    print("å¼€å§‹ç‰¹å¾ç›¸å…³æ€§è¿‡æ»¤...")
    corr_matrix = data[all_features].corr().abs()
    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
    to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]
    if to_drop:
        print(f"æ£€æµ‹åˆ°é«˜ç›¸å…³ç‰¹å¾ {len(to_drop)} ä¸ªï¼Œå°†è¿›è¡Œå‰”é™¤ã€‚")
    else:
        print("æœªæ£€æµ‹åˆ°é«˜ç›¸å…³ç‰¹å¾ã€‚")
    all_features_filtered = [f for f in all_features if f not in to_drop]
    X = data[all_features_filtered]
    print(f"è¿‡æ»¤åç‰¹å¾æ•°é‡ï¼š{len(all_features_filtered)}")

    # æ•°æ®æ ‡å‡†åŒ–
    print("æ ‡å‡†åŒ–æ•°æ®...")
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X).astype(np.float32)

    # é€‰æ‹©å¹¶åº”ç”¨è¿‡é‡‡æ ·æ–¹æ³•
    print(f"åº”ç”¨è¿‡é‡‡æ ·æ–¹æ³•ï¼š{oversample_method}")
    if oversample_method == 'è¿‡é‡‡æ ·':
        sampler = SMOTE(random_state=42)
    elif oversample_method == 'ç±»åˆ«æƒé‡':
        sampler = None
    else:
        raise ValueError(f"æœªçŸ¥çš„è¿‡é‡‡æ ·æ–¹æ³•: {oversample_method}")

    if sampler is not None:
        if classifier_name == 'Transformer':
            print("ä¸º Transformer åˆ›å»ºåºåˆ—æ•°æ®...")
            X_seq, y_seq = create_sequences(X_scaled, data[label_column].astype(np.int64), window_size=window_size)
            print(f"åºåˆ—æ•°æ®å½¢çŠ¶: X={X_seq.shape}, y={y_seq.shape}")
            
            # é‡å¡‘æ•°æ®ä»¥é€‚åº” SMOTE
            X_reshaped = X_seq.reshape(X_seq.shape[0], -1)
            print("å¯¹åºåˆ—æ•°æ®è¿›è¡Œè¿‡é‡‡æ ·å¤„ç†...")
            X_resampled, y_resampled = sampler.fit_resample(X_reshaped, y_seq)
            
            X_resampled = X_resampled.reshape(-1, window_size, X_seq.shape[2])
            print(f"è¿‡é‡‡æ ·åæ•°æ®å½¢çŠ¶: X={X_resampled.shape}, y={y_resampled.shape}")
        else:
            print("å¯¹æ•°æ®è¿›è¡Œè¿‡é‡‡æ ·å¤„ç†...")
            X_resampled, y_resampled = sampler.fit_resample(X_scaled, data[label_column].astype(np.int64))
            print(f"æ•°æ®å½¢çŠ¶: X={X_resampled.shape}, y={y_resampled.shape}")
    else:
        print("ä¸è¿›è¡Œè¿‡é‡‡æ ·ï¼Œä½¿ç”¨åŸå§‹æ•°æ®ã€‚")
        X_resampled, y_resampled = X_scaled, data[label_column].astype(np.int64).values

    # è®¡ç®—ç±»åˆ«æƒé‡
    if oversample_method == 'ç±»åˆ«æƒé‡':
        class_weights_array = get_class_weights(y_resampled)
        if isinstance(class_weights_array, torch.Tensor):
            class_weights_array = class_weights_array.float()
    else:
        class_weights_array = class_weight

    # åˆ’åˆ†è®­ç»ƒæµ‹è¯•é›†
    X_train, X_test, y_train, y_test = train_test_split(
        X_resampled, y_resampled, test_size=0.2, 
        random_state=42, stratify=y_resampled
    )
    print(f"è®­ç»ƒé›†å¤§å°: {X_train.shape}, æµ‹è¯•é›†å¤§å°: {X_test.shape}")

    # è·å–åˆ†ç±»å™¨
    num_features = X_train.shape[-1] if classifier_name == 'Transformer' else X_train.shape[1]
    if classifier_name == 'Transformer':
        clf_name, clf = get_transformer_classifier(
            num_features=num_features,
            window_size=window_size,
            class_weights=class_weights_array
        )
    elif classifier_name == 'æ·±åº¦å­¦ä¹ ':
        clf_name, clf = get_mlp_classifier(
            input_dim=num_features,
            class_weights=class_weights_array
        )
    else:
        clf_name, clf = get_classifier(
            classifier_name,
            num_features=num_features,
            window_size=window_size,
            class_weight='balanced' if oversample_method == 'ç±»åˆ«æƒé‡' else class_weight
        )

    # ç½‘æ ¼æœç´¢
    print(f"æ­£åœ¨ä¸ºåˆ†ç±»å™¨ {clf_name} è¿›è¡Œ GridSearchCV è°ƒå‚...")
    param_grid = {}
    if clf_name == 'rf':
        param_grid = {
            'n_estimators': [100, 200],
            'max_depth': [None, 10],
            'min_samples_split': [2, 5],
            'min_samples_leaf': [1, 2]
        }
    elif clf_name == 'svc':
        param_grid = {
            'C': [0.1, 1],
            'gamma': ['scale', 'auto']
        }
    elif clf_name == 'lr':
        param_grid = {
            'C': [0.01, 0.1, 1],
            'penalty': ['l2']
        }
    elif clf_name == 'gb':
        param_grid = {
            'n_estimators': [100, 200],
            'learning_rate': [0.01, 0.1],
            'max_depth': [3, 5]
        }
    elif clf_name in ['transformer', 'mlp']:
        param_grid = {
            'lr': [1e-3],
            'max_epochs': [10]
        }
        if clf_name == 'transformer':
            param_grid['module__hidden_dim'] = [64]
    else:
        param_grid = {}

    grid_search = GridSearchCV(
        estimator=clf,
        param_grid=param_grid,
        cv=3,
        n_jobs=1 if clf_name in ['transformer', 'mlp'] else -1,
        scoring='f1',
        verbose=1,
        error_score='raise'
    )

    try:
        grid_search.fit(X_train, y_train)
        print(f"æœ€ä½³å‚æ•°: {grid_search.best_params_}")
        print(f"æœ€ä½³å¾—åˆ†: {grid_search.best_score_:.4f}")
        best_estimator = grid_search.best_estimator_
    except Exception as e:
        print(f"GridSearchCV å¤±è´¥: {e}")
        raise

    # ç‰¹å¾é€‰æ‹©
    print("å¼€å§‹ç‰¹å¾é€‰æ‹©...")
    feature_selector = None
    selected_features = all_features_filtered.copy()

    if clf_name in ['rf', 'gb'] and n_features_selected != 'auto':
        feature_selector = RFE(
            estimator=best_estimator,
            n_features_to_select=int(n_features_selected),
            step=1
        )
        # transformer ä¸åŒç»“æ„æ— éœ€ RFE
        if not clf_name == 'transformer':
            feature_selector.fit(X_train, y_train)
            selected_features = [
                all_features_filtered[i] 
                for i in range(len(all_features_filtered)) 
                if feature_selector.support_[i]
            ]
            print(f"RFEé€‰æ‹©çš„ç‰¹å¾æ•°é‡ï¼š{len(selected_features)}")
    else:
        print(f"{clf_name} ä¸è¿›è¡Œç‰¹å¾é€‰æ‹©ï¼Œä½¿ç”¨å…¨éƒ¨ç‰¹å¾")
        feature_selector = FunctionTransformer(func=identity, validate=False)

    # è¯„ä¼°æ¨¡å‹
    if isinstance(best_estimator, NeuralNetClassifier):
        if feature_selector is not None and not isinstance(feature_selector, FunctionTransformer):
            X_test_selected = feature_selector.transform(X_test)
            logits = best_estimator.predict_proba(X_test_selected)
        else:
            logits = best_estimator.predict_proba(X_test)
            
        if isinstance(logits, np.ndarray) and logits.ndim == 2:
            y_proba = logits[:, 1]
        else:
            y_proba = F.softmax(torch.tensor(logits), dim=1)[:, 1].numpy()
    else:
        if feature_selector is not None:
            X_test_selected = feature_selector.transform(X_test)
            y_proba = best_estimator.predict_proba(X_test_selected)[:, 1]
        else:
            y_proba = best_estimator.predict_proba(X_test)[:, 1]

    best_thresh = optimize_threshold(y_test, y_proba)
    y_pred = (y_proba > best_thresh).astype(int)

    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    pr_auc = average_precision_score(y_test, y_proba)
    mcc = matthews_corrcoef(y_test, y_pred)
    roc_auc = roc_auc_score(y_test, y_proba)

    print("\næ¨¡å‹è¯„ä¼°ç»“æœï¼š")
    print(classification_report(y_test, y_pred))
    print("\næ··æ·†çŸ©é˜µï¼š")
    print(confusion_matrix(y_test, y_pred))
    print(f"\nROC AUC: {roc_auc:.4f}")
    print(f"PR AUC: {pr_auc:.4f}")
    print(f"MCC: {mcc:.4f}")

    metrics = {
        'ROC AUC': roc_auc,
        'PR AUC': pr_auc,
        'Precision': precision,
        'Recall': recall,
        'MCC': mcc
    }

    return (
        best_estimator, 
        scaler, 
        feature_selector, 
        selected_features, 
        all_features_filtered, 
        grid_search.best_score_, 
        metrics, 
        best_thresh
    )

##############################################################################
#                       å¹¶è¡Œè®­ç»ƒ Peak & Trough ä¸»å‡½æ•°
##############################################################################
@st.cache_resource
def train_model(
    df_preprocessed, N, all_features, classifier_name, 
    mixture_depth, n_features_selected, oversample_method, window_size=10
):
    print("å¼€å§‹è®­ç»ƒæ¨¡å‹...")
    data = df_preprocessed.copy()
    print(f"é¢„å¤„ç†åæ•°æ®é•¿åº¦: {len(data)}")

    labels = ['Peak', 'Trough']
    
    results = Parallel(n_jobs=-1)(
        delayed(train_model_for_label)(
            data, N, label, all_features, classifier_name, 
            n_features_selected, window_size, 
            oversample_method, 
            class_weight='balanced' if oversample_method == 'ç±»åˆ«æƒé‡' else None
        )
        for label in labels
    )

    peak_results = results[0]
    trough_results = results[1]
    return peak_results, trough_results

##############################################################################
#                       å¹³æ»‘éªŒè¯å‡½æ•°
##############################################################################
def smooth_predictions(pred_series, min_days_between_predictions=20):
    peaks = []
    last_peak = -min_days_between_predictions
    for i in range(len(pred_series)):
        if pred_series[i] == 1 and (i - last_peak) >= min_days_between_predictions:
            peaks.append(i)
            last_peak = i
    pred = np.zeros(len(pred_series))
    pred[peaks] = 1
    return pred

##############################################################################
#                       éªŒè¯æ–°æ•°æ®çš„å‡½æ•°
##############################################################################
@st.cache_data
def predict_new_data(new_df,
                     _peak_model,
                     _peak_scaler,
                     _peak_selector,
                     all_features_peak,
                     peak_threshold,
                     _trough_model,
                     _trough_scaler,
                     _trough_selector,
                     all_features_trough,
                     trough_threshold,
                     N, mixture_depth=3, min_days_between_predictions=20):
    print("å¼€å§‹éªŒè¯æ–°æ•°æ®...")
    
    # é¢„å¤„ç†æ•°æ®ï¼ˆmark_labels=Falseï¼‰
    data_preprocessed, _ = preprocess_data(new_df, N, mixture_depth=mixture_depth, mark_labels=False)
    print(f"é¢„å¤„ç†åæ•°æ®é•¿åº¦: {len(data_preprocessed)}")

    # ========== Peak éªŒè¯ ==========
    print("\nå¼€å§‹PeakéªŒè¯...")
    missing_features_peak = [f for f in all_features_peak if f not in data_preprocessed.columns]
    if missing_features_peak:
        print(f"å¡«å……ç¼ºå¤±ç‰¹å¾(Peak): {missing_features_peak}")
        for feature in missing_features_peak:
            data_preprocessed[feature] = 0
            
    X_new_peak = data_preprocessed[all_features_peak].fillna(0)
    X_new_peak_scaled = _peak_scaler.transform(X_new_peak).astype(np.float32)
    print(f"Peakæ•°æ®å½¢çŠ¶: {X_new_peak_scaled.shape}")

    if isinstance(_peak_model, NeuralNetClassifier) and \
       isinstance(_peak_model.module_, TransformerClassifier):
        print("åˆ›å»ºPeakåºåˆ—æ•°æ®...")
        X_new_seq_peak = create_sequences(X_new_peak_scaled, y=None, window_size=10)  # å›ºå®šä¸º10
        print(f"Peakåºåˆ—æ•°æ®å½¢çŠ¶: {X_new_seq_peak.shape}")

        batch_size = 64
        predictions = []
        _peak_model.module_.eval()
        
        with torch.no_grad():
            for i in range(0, len(X_new_seq_peak), batch_size):
                batch = torch.from_numpy(X_new_seq_peak[i:i+batch_size]).float()
                batch = batch.to(_peak_model.device)
                outputs = _peak_model.module_(batch)
                probs = torch.softmax(outputs, dim=1)[:, 1]
                predictions.append(probs.cpu().numpy())

        all_probas = np.concatenate(predictions)
        peak_probas = np.zeros(len(data_preprocessed))
        peak_probas[10-1:] = all_probas  # window_size=10
        
    else:
        if isinstance(_peak_model, NeuralNetClassifier):
            if _peak_selector is not None and not isinstance(_peak_selector, FunctionTransformer):
                X_new_peak_selected = _peak_selector.transform(X_new_peak_scaled)
                logits = _peak_model.predict_proba(X_new_peak_selected)
            else:
                logits = _peak_model.predict_proba(X_new_peak_scaled)
            if logits.ndim == 2:
                peak_probas = logits[:, 1]
            else:
                peak_probas = torch.sigmoid(torch.tensor(logits)).numpy()
        else:
            if _peak_selector is not None:
                X_new_peak_selected = _peak_selector.transform(X_new_peak_scaled)
                peak_probas = _peak_model.predict_proba(X_new_peak_selected)[:, 1]
            else:
                peak_probas = _peak_model.predict_proba(X_new_peak_scaled)[:, 1]

    peak_preds = (peak_probas > peak_threshold).astype(int)
    data_preprocessed['Peak_Probability'] = peak_probas
    data_preprocessed['Peak_Prediction'] = peak_preds

    # ========== Trough éªŒè¯ ==========
    print("\nå¼€å§‹TroughéªŒè¯...")
    missing_features_trough = [f for f in all_features_trough if f not in data_preprocessed.columns]
    if missing_features_trough:
        print(f"å¡«å……ç¼ºå¤±ç‰¹å¾(Trough): {missing_features_trough}")
        for feature in missing_features_trough:
            data_preprocessed[feature] = 0
            
    X_new_trough = data_preprocessed[all_features_trough].fillna(0)
    X_new_trough_scaled = _trough_scaler.transform(X_new_trough).astype(np.float32)
    print(f"Troughæ•°æ®å½¢çŠ¶: {X_new_trough_scaled.shape}")

    if isinstance(_trough_model, NeuralNetClassifier) and \
       isinstance(_trough_model.module_, TransformerClassifier):
        print("åˆ›å»ºTroughåºåˆ—æ•°æ®...")
        X_new_seq_trough = create_sequences(X_new_trough_scaled, y=None, window_size=10)  # å›ºå®šä¸º10
        print(f"Troughåºåˆ—æ•°æ®å½¢çŠ¶: {X_new_seq_trough.shape}")

        batch_size = 64
        predictions = []
        _trough_model.module_.eval()
        
        with torch.no_grad():
            for i in range(0, len(X_new_seq_trough), batch_size):
                batch = torch.from_numpy(X_new_seq_trough[i:i+batch_size]).float()
                batch = batch.to(_trough_model.device)
                outputs = _trough_model.module_(batch)
                probs = torch.softmax(outputs, dim=1)[:, 1]
                predictions.append(probs.cpu().numpy())

        all_probas = np.concatenate(predictions)
        trough_probas = np.zeros(len(data_preprocessed))
        trough_probas[10-1:] = all_probas  # window_size=10
        
    else:
        if isinstance(_trough_model, NeuralNetClassifier):
            if _trough_selector is not None and not isinstance(_trough_selector, FunctionTransformer):
                X_new_trough_selected = _trough_selector.transform(X_new_trough_scaled)
                logits = _trough_model.predict_proba(X_new_trough_selected)
            else:
                logits = _trough_model.predict_proba(X_new_trough_scaled)
            if logits.ndim == 2:
                trough_probas = logits[:, 1]
            else:
                trough_probas = torch.sigmoid(torch.tensor(logits)).numpy()
        else:
            if _trough_selector is not None:
                X_new_trough_selected = _trough_selector.transform(X_new_trough_scaled)
                trough_probas = _trough_model.predict_proba(X_new_trough_selected)[:, 1]
            else:
                trough_probas = _trough_model.predict_proba(X_new_trough_scaled)[:, 1]

    trough_preds = (trough_probas > trough_threshold).astype(int)
    data_preprocessed['Trough_Probability'] = trough_probas
    data_preprocessed['Trough_Prediction'] = trough_preds

    # åå¤„ç†ï¼šæŒ‡å®šå¤©æ•°å†…ä¸é‡å¤éªŒè¯
    print("\nè¿›è¡Œåå¤„ç†...")
    data_preprocessed.index = data_preprocessed.index.astype(str)
    
    # ä½¿ç”¨å¹³æ»‘é¢„æµ‹å‡½æ•°
    data_preprocessed['Peak_Prediction'] = smooth_predictions(data_preprocessed['Peak_Prediction'], min_days_between_predictions)
    data_preprocessed['Trough_Prediction'] = smooth_predictions(data_preprocessed['Trough_Prediction'], min_days_between_predictions)

    return data_preprocessed

##############################################################################
#                       ç»˜å›¾å‡½æ•°ï¼ˆä½¿ç”¨Plotlyï¼‰
##############################################################################
def plot_candlestick_plotly(data, symbol_code, start_date, end_date, peaks=None, troughs=None, prediction=False, selected_classifiers=None):
    if prediction and selected_classifiers:
        classifiers_str = ", ".join(selected_classifiers)
        title = f"{symbol_code} {start_date} è‡³ {end_date} åŸºç¡€æ¨¡å‹: {classifiers_str}"
    else:
        title = f"{symbol_code} {start_date} è‡³ {end_date}"

    if not isinstance(data.index, pd.DatetimeIndex):
        try:
            data.index = pd.to_datetime(data.index)
        except Exception as e:
            raise ValueError(f"data.index æ— æ³•è½¬æ¢ä¸ºæ—¥æœŸæ ¼å¼: {e}")
    data.index = data.index.strftime('%Y-%m-%d')

    if peaks is not None and not peaks.empty:
        if not isinstance(peaks.index, pd.DatetimeIndex):
            try:
                peaks.index = pd.to_datetime(peaks.index)
            except Exception as e:
                raise ValueError(f"peaks.index æ— æ³•è½¬æ¢ä¸ºæ—¥æœŸæ ¼å¼: {e}")
        peaks.index = peaks.index.strftime('%Y-%m-%d')

    if troughs is not None and not troughs.empty:
        if not isinstance(troughs.index, pd.DatetimeIndex):
            try:
                troughs.index = pd.to_datetime(troughs.index)
            except Exception as e:
                raise ValueError(f"troughs.index æ— æ³•è½¬æ¢ä¸ºæ—¥æœŸæ ¼å¼: {e}")
        troughs.index = troughs.index.strftime('%Y-%m-%d')

    fig = make_subplots(
        rows=2, cols=1,
        shared_xaxes=True,
        vertical_spacing=0.02,
        row_heights=[0.7, 0.3],
        specs=[[{"type": "candlestick"}],[{"type": "bar"}]]
    )

    fig.add_trace(go.Candlestick(
        x=data.index,
        open=data['Open'],
        high=data['High'],
        low=data['Low'],
        close=data['Close'],
        name=symbol_code,
        increasing=dict(line=dict(color='red')),
        decreasing=dict(line=dict(color='green')),
        hoverinfo='x+y+text',
    ), row=1, col=1)

    if 'Volume' in data.columns:
        volume_colors = ['red' if row['Close'] > row['Open'] else 'green' for _, row in data.iterrows()]
        fig.add_trace(go.Bar(
            x=data.index,
            y=data['Volume'],
            marker_color=volume_colors,
            name='æˆäº¤é‡',
            hoverinfo='x+y'
        ), row=2, col=1)

    if peaks is not None and not peaks.empty:
        marker_y_peaks = peaks['High'] * 1.02
        marker_x_peaks = peaks.index
        color_peak = 'green'
        label_peak = 'å±€éƒ¨é«˜ç‚¹' if not prediction else 'éªŒè¯é«˜ç‚¹'
        fig.add_trace(go.Scatter(
            x=marker_x_peaks,
            y=marker_y_peaks,
            mode='text',
            text='W',
            textfont=dict(color=color_peak, size=20),
            name=label_peak
        ), row=1, col=1)

    if troughs is not None and not troughs.empty:
        marker_y_troughs = troughs['Low'] * 0.98
        marker_x_troughs = troughs.index
        color_trough = 'red'
        label_trough = 'å±€éƒ¨ä½ç‚¹' if not prediction else 'éªŒè¯ä½ç‚¹'
        fig.add_trace(go.Scatter(
            x=marker_x_troughs,
            y=marker_y_troughs,
            mode='text',
            text='D',
            textfont=dict(color=color_trough, size=20),
            name=label_trough
        ), row=1, col=1)

    fig.update_layout(
        title=title,
        xaxis=dict(
            title="æ—¥æœŸ",
            type="category",
            tickangle=45,
            tickmode="auto",
            nticks=10
        ),
        xaxis2=dict(
            title="æ—¥æœŸ",
            type="category",
            tickangle=45,
            tickmode="auto",
            nticks=10
        ),
        yaxis_title="ä»·æ ¼",
        xaxis_rangeslider_visible=False,
        hovermode='x unified',
        template='plotly_white',
        showlegend=True,
        height=800,
        font=dict(
            family="Microsoft YaHei, SimHei",
            size=14,
            color="black"
        )
    )
    return fig

##############################################################################
#                       ä¸‹è½½æŒ‰é’®å‡½æ•°
##############################################################################
def download_link(df, filename, text):
    csv = df.to_csv(index=False)
    b64 = base64.b64encode(csv.encode()).decode()  # ç¼–ç ä¸ºbase64
    href = f'<a href="data:file/csv;base64,{b64}" download="{filename}">{text}</a>'
    return href

##############################################################################
#                       Streamlit ä¸»å‡½æ•°ï¼ˆå¸ƒå±€è°ƒæ•´åï¼‰
##############################################################################
def main():
    st.set_page_config(page_title="æŒ‡æ•°å±€éƒ¨é«˜ä½ç‚¹éªŒè¯", layout="wide")
    st.title("æŒ‡æ•°å±€éƒ¨é«˜ä½ç‚¹éªŒè¯")

    # ä½¿ç”¨ Sidebar åˆ†åŒºï¼Œå°†æ¨¡å‹è®­ç»ƒä¸æ¨¡å‹éªŒè¯åˆ†åˆ«æ”¾åœ¨ä¸åŒçš„å¯¼èˆªé€‰é¡¹ä¸­
    menu = st.sidebar.radio("å¯¼èˆª", ["ğŸ“ˆ æ¨¡å‹è®­ç»ƒ", "ğŸ” æ¨¡å‹éªŒè¯"])

    if menu == "ğŸ“ˆ æ¨¡å‹è®­ç»ƒ":
        train_section()
    elif menu == "ğŸ” æ¨¡å‹éªŒè¯":
        predict_section()

##############################################################################
#                       è®­ç»ƒéƒ¨åˆ†å‡½æ•°
##############################################################################
def train_section():
    st.header("æ¨¡å‹è®­ç»ƒæµç¨‹")
    st.markdown("""
    **æœºå™¨å­¦ä¹ ä¸»è¦æ­¥éª¤ï¼š**  
    1. **æ•°æ®æ ‡æ³¨**ï¼šæ ‡æ³¨é«˜ä½ç‚¹ï¼Œä½œä¸ºè®­ç»ƒç›®æ ‡ã€‚  
    2. **é€‰æ‹©åŸºç¡€æ¨¡å‹**ï¼šæ¨èä½¿ç”¨æ·±åº¦å­¦ä¹ å’Œæ¢¯åº¦æå‡ã€‚  
    3. **å‡è¡¡ç±»åˆ«**ï¼šè®­ç»ƒæ—¶é«˜ä½ç‚¹ä¸éé«˜ä½ç‚¹æ•°é‡éœ€æ¥è¿‘1ï¼š1ï¼Œéœ€è¿‡é‡‡æ ·æˆ–æé«˜é«˜ä½ç‚¹æƒé‡ã€‚  
    4. **ç‰¹å¾é€‰æ‹©**ï¼šç‰¹å¾å³æŒ‡æ ‡ï¼Œå†…ç½®32ä¸ªï¼Œæ··åˆåå°†ç”Ÿæˆæ›´å¤šæŒ‡æ ‡ã€‚æŒ‰ä¸è¡Œæƒ…ç›¸å…³ç¨‹åº¦ç”±é«˜åˆ°åº•æ’åºï¼Œå¯æ‰‹åŠ¨é€‰æ‹©æ•°é‡ã€‚
    """)

    with st.expander("ğŸ› ï¸ æ•°æ®ä¸å‚æ•°è®¾ç½®", expanded=True):
        with st.form("train_form"):
            # ========== ä¿®æ”¹éƒ¨åˆ†å¼€å§‹ ==========
            symbol_type_display = st.radio(
                "1ï¸âƒ£ ä»£ç ç±»å‹",
                options=["è‚¡ç¥¨", "æŒ‡æ•°"],
                index=1,  # é»˜è®¤é€‰æ‹©â€œæŒ‡æ•°â€
                help="é€‰æ‹©è¾“å…¥çš„æ˜¯è‚¡ç¥¨ä»£ç è¿˜æ˜¯æŒ‡æ•°ä»£ç ã€‚"
            )

            # å°†ä¸­æ–‡é€‰æ‹©è½¬æ¢ä¸ºè‹±æ–‡
            symbol_type = 'stock' if symbol_type_display == "è‚¡ç¥¨" else 'index'

            # åŠ¨æ€è®¾ç½®é»˜è®¤ symbol_code
            if symbol_type == "stock":
                default_symbol_code = "000001.SZ"
            else:
                default_symbol_code = "000001.SH"

            symbol_code = st.text_input(
                "2ï¸âƒ£ è‚¡ç¥¨æˆ–æŒ‡æ•°ä»£ç ", 
                value=default_symbol_code, 
                help="è¯·è¾“å…¥è‚¡ç¥¨ä»£ç ï¼ˆå¦‚ 000001.SZï¼‰æˆ–æŒ‡æ•°ä»£ç ï¼ˆå¦‚ 000300.SHï¼‰ã€‚"
            )
            # ========== ä¿®æ”¹éƒ¨åˆ†ç»“æŸ ==========

            # è®­ç»ƒé›†æ—¥æœŸ
            st.markdown("ğŸ“… **è®­ç»ƒé›†æ—¥æœŸ**")
            col_train_date1, col_train_date2 = st.columns(2)
            with col_train_date1:
                start_date = st.date_input(
                    "è®­ç»ƒå¼€å§‹æ—¥æœŸ", 
                    datetime.strptime("2000-01-01", "%Y-%m-%d"), 
                    key='train_start_date',
                    help="é€‰æ‹©è®­ç»ƒæ•°æ®çš„å¼€å§‹æ—¥æœŸã€‚"
                )
            with col_train_date2:
                end_date = st.date_input(
                    "è®­ç»ƒç»“æŸæ—¥æœŸ", 
                    datetime.strptime("2020-12-31", "%Y-%m-%d"), 
                    key='train_end_date',
                    help="é€‰æ‹©è®­ç»ƒæ•°æ®çš„ç»“æŸæ—¥æœŸã€‚"
                )

            N = st.number_input(
                "3ï¸âƒ£ æ ‡æ³¨é«˜ä½ç‚¹é—´çš„æœ€å°é—´éš” (N)", 
                min_value=1, 
                max_value=1000000, 
                value=30, 
                help="ç”¨äºæ•°æ®é¢„å¤„ç†çš„çª—å£é•¿åº¦ï¼Œå†³å®šå¦‚ä½•æ ‡æ³¨é«˜ä½ç‚¹ã€‚"
            )

            # é€‰æ‹©åŸºç¡€æ¨¡å‹
            available_classifiers = ['éšæœºæ£®æ—', 'æ”¯æŒå‘é‡æœº', 'é€»è¾‘å›å½’', 'æ¢¯åº¦æå‡', 'Transformer', 'æ·±åº¦å­¦ä¹ ']
            classifier_name = st.selectbox(
                "4ï¸âƒ£ é€‰æ‹©åŸºç¡€æ¨¡å‹", 
                available_classifiers, 
                help="é€‰æ‹©ç”¨äºè®­ç»ƒçš„åˆ†ç±»å™¨æ¨¡å‹ã€‚"
            )

            # å› å­æ··åˆæ·±åº¦
            mixture_depth = st.slider(
                "5ï¸âƒ£ å› å­æ··åˆæ·±åº¦", 
                min_value=1, 
                max_value=3, 
                value=1, 
                help="é€‰æ‹©å› å­æ··åˆçš„æ·±åº¦ã€‚"
            )

            # è¿‡é‡‡æ ·æ–¹æ³•é€‰æ‹©
            oversample_methods = [
                'è¿‡é‡‡æ ·',
                'ç±»åˆ«æƒé‡'
            ]
            oversample_method = st.selectbox(
                "6ï¸âƒ£ å¤„ç†ç±»åˆ«ä¸å‡è¡¡çš„æ–¹æ³•", 
                oversample_methods, 
                help="é€‰æ‹©ç”¨äºå¤„ç†ç±»åˆ«ä¸å‡è¡¡çš„æ–¹æ³•ã€‚"
            )

            # ç‰¹å¾é€‰æ‹©
            st.markdown("ğŸ” **ç‰¹å¾é€‰æ‹©**")
            auto_feature = st.checkbox(
                "è‡ªåŠ¨é€‰æ‹©ç‰¹å¾æ•°é‡ï¼ˆä»…å¯¹éšæœºæ£®æ—ã€æ¢¯åº¦æå‡æœ‰æ•ˆï¼‰", 
                value=True
            )
            if auto_feature:
                n_features_selected = 'auto'
            else:
                n_features_selected = st.number_input(
                    "é€‰æ‹©ç‰¹å¾æ•°é‡", 
                    min_value=1, 
                    max_value=1000, 
                    value=20, 
                    help="æ‰‹åŠ¨é€‰æ‹©ç‰¹å¾çš„æ•°é‡ã€‚"
                )

            submit_train = st.form_submit_button("æäº¤å‚æ•°")
        
        if submit_train:
            st.session_state['train_params'] = {
                'symbol_type': symbol_type,  # ä½¿ç”¨è‹±æ–‡çš„ 'stock' æˆ– 'index'
                'symbol_code': symbol_code,
                'start_date': start_date,
                'end_date': end_date,
                'N': N,
                'classifier_name': classifier_name,
                'mixture_depth': mixture_depth,
                'oversample_method': oversample_method,
                'n_features_selected': n_features_selected
            }
            st.success("å‚æ•°å·²æäº¤ï¼Œè¯·ç‚¹å‡»ä¸‹æ–¹ã€è®­ç»ƒæ¨¡å‹ã€æŒ‰é’®å¼€å§‹è®­ç»ƒã€‚")

            # ========== è·å–å¹¶æ˜¾ç¤ºåç§° ==========
            try:
                if symbol_type == "stock":
                    stock_info = pro.stock_basic(ts_code=symbol_code, fields='ts_code,name')
                    if not stock_info.empty:
                        stock_name = stock_info.iloc[0]['name']
                        st.markdown(f"**è‚¡ç¥¨åç§°ï¼š** {stock_name}")
                    else:
                        st.warning("æ— æ³•è·å–è‚¡ç¥¨åç§°ï¼Œè¯·æ£€æŸ¥è‚¡ç¥¨ä»£ç ã€‚")
                else:
                    index_info = pro.index_basic(ts_code=symbol_code, fields='ts_code,name')
                    if not index_info.empty:
                        index_name = index_info.iloc[0]['name']
                        st.markdown(f"**æŒ‡æ•°åç§°ï¼š** {index_name}")
                    else:
                        st.warning("æ— æ³•è·å–æŒ‡æ•°åç§°ï¼Œè¯·æ£€æŸ¥æŒ‡æ•°ä»£ç ã€‚")
            except Exception as e:
                st.error(f"è·å–åç§°å¤±è´¥ï¼š{e}")

            # ========== è°ƒè¯•è¾“å‡ºï¼šå°è¯•è·å–æ•°æ®å¹¶æ˜¾ç¤º ==========
            try:
                #st.write("æ­£åœ¨å°è¯•è·å–æ•°æ®...")
                data = read_day_from_tushare(symbol_code, symbol_type=symbol_type)
                st.write(f"è·å–åˆ°çš„æ•°æ®è¡Œæ•°: {len(data)}")
                st.write(f"æ•°æ®é¢„å¤„ç†å®Œæˆï¼Œè¯·ç‚¹å‡»æŒ‰é’®å¼€å§‹è®­ç»ƒ")
                if data.empty:
                    st.warning("è·å–çš„æ•°æ®ä¸ºç©ºï¼Œè¯·æ£€æŸ¥ä»£ç ç±»å‹å’Œä»£ç æ˜¯å¦æ­£ç¡®ã€‚")
            except Exception as e:
                st.error(f"è·å–æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")

    st.markdown("---")
    st.subheader("ğŸš€ å¼€å§‹è®­ç»ƒ")
    st.markdown("è¯·ç‚¹å‡»ä¸‹æ–¹æŒ‰é’®å¼€å§‹è®­ç»ƒæ¨¡å‹ã€‚")
    
    if st.button("è®­ç»ƒæ¨¡å‹"):
        if 'train_params' not in st.session_state or not st.session_state['train_params']:
            st.error("è¯·åœ¨ä¸Šæ–¹æäº¤è®­ç»ƒå‚æ•°ã€‚")
        else:
            params = st.session_state['train_params']
            symbol_type = params['symbol_type']
            symbol_code = params['symbol_code']
            start_date = params['start_date']
            end_date = params['end_date']
            N = params['N']
            classifier_name = params['classifier_name']
            mixture_depth = params['mixture_depth']
            oversample_method = params['oversample_method']
            n_features_selected = params['n_features_selected']

            # å‚æ•°éªŒè¯
            if not all([symbol_code, N, mixture_depth, classifier_name]):
                st.error("è¯·å¡«å†™æ‰€æœ‰è®­ç»ƒå‚æ•°ã€‚")
            elif start_date > end_date:
                st.error("å¼€å§‹æ—¥æœŸä¸èƒ½æ™šäºç»“æŸæ—¥æœŸã€‚")
            else:
                with st.spinner("æ­£åœ¨è¯»å–æ•°æ®å¹¶è¿›è¡Œé¢„å¤„ç†..."):
                    try:
                        # è¯»å–æ•°æ®
                        data = read_day_from_tushare(symbol_code, symbol_type=symbol_type)
                        if data.empty:
                            st.error("é€šè¿‡ Tushare è·å–çš„æ•°æ®ä¸ºç©ºï¼Œè¯·æ£€æŸ¥ä»£ç ç±»å‹å’Œä»£ç ã€‚")
                            st.stop()
                        
                        # æ ¹æ®æ—¥æœŸèŒƒå›´æˆªå–æ•°æ®
                        df = select_time(data, start_date.strftime("%Y%m%d"), end_date.strftime("%Y%m%d"))
                        if df.empty:
                            st.error("è®­ç»ƒé›†ä¸ºç©ºï¼Œè¯·æ£€æŸ¥æ—¥æœŸèŒƒå›´å’Œä»£ç ã€‚")
                            st.stop()
                        
                        # é¢„å¤„ç†æ•°æ®
                        df_preprocessed, all_features = preprocess_data(
                            df, N, mixture_depth, mark_labels=True
                        )
                        
                        st.success("æ•°æ®é¢„å¤„ç†å®Œæˆ")
                        
                        # ========== æ˜¾ç¤ºç­›é€‰åçš„æ•°æ®è¡Œæ•°å’Œç‰¹å¾æ•°é‡ ==========
                        num_rows = len(df_preprocessed)
                        num_features = len(all_features)
                        col1, col2 = st.columns(2)
                        with col1:
                            st.write(f"**ç­›é€‰åçš„æ•°æ®è¡Œæ•°ï¼š** {num_rows}")
                        with col2:
                            st.write(f"**ç‰¹å¾æ•°é‡ï¼š** {num_features}")
                        
                    except AssertionError as ae:
                        st.error(f"æ•°æ®å¤„ç†æ–­è¨€å¤±è´¥ï¼š{ae}")
                        st.stop()
                    except Exception as e:
                        st.error(f"æ•°æ®å¤„ç†å¤±è´¥ï¼š{e}")
                        st.stop()

                # ========== æ˜¾ç¤ºæ ‡æ³¨å¥½çš„å›¾è¡¨ ==========
                st.subheader("ğŸ“Š é¢„å¤„ç†åçš„æ ‡æ³¨å›¾è¡¨")
                try:
                    peaks = df_preprocessed[df_preprocessed['Peak'] == 1] if 'Peak' in df_preprocessed.columns else pd.DataFrame()
                    troughs = df_preprocessed[df_preprocessed['Trough'] == 1] if 'Trough' in df_preprocessed.columns else pd.DataFrame()
                    symbol_display = symbol_code
                    fig_initial = plot_candlestick_plotly(
                        df_preprocessed, symbol_display, start_date.strftime("%Y%m%d"), end_date.strftime("%Y%m%d"),
                        peaks=peaks, troughs=troughs, prediction=False, selected_classifiers=None
                    )
                    st.plotly_chart(fig_initial, use_container_width=True)
                except Exception as e:
                    st.error(f"ç»˜åˆ¶æ ‡æ³¨å›¾è¡¨å¤±è´¥ï¼š{e}")
                    st.stop()

                # ========== å¼€å§‹æ¨¡å‹è®­ç»ƒ ==========
                with st.spinner("æ¨¡å‹è®­ç»ƒä¸­ï¼Œè¯·ç¨å€™..."):
                    try:
                        # å¹¶è¡Œè®­ç»ƒ peak / trough
                        peak_results, trough_results = train_model(
                            df_preprocessed, N, all_features, classifier_name, 
                            mixture_depth, n_features_selected, oversample_method
                        )
                        
                        (peak_model, peak_scaler, peak_selector, peak_selected_features, all_features_peak, peak_best_score,
                         peak_metrics, peak_threshold) = peak_results
                        (trough_model, trough_scaler, trough_selector, trough_selected_features, all_features_trough,
                         trough_best_score, trough_metrics, trough_threshold) = trough_results
                        
                        st.success("æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
                        
                        # è®­ç»ƒç»“æœå±•ç¤º
                        st.subheader("ğŸ“ˆ è®­ç»ƒç»“æœä¸è¯„ä¼°æŒ‡æ ‡")
                        c1, c2 = st.columns(2)
                        with c1:
                            st.markdown("#### é«˜ç‚¹è¯†åˆ«")
                            st.write(f"**æœ€ä½³å¾—åˆ†ï¼š** {peak_best_score:.2f}")
                            st.write("**è¯„ä¼°æŒ‡æ ‡ï¼š**")
                            # å°†æŒ‡æ ‡ä¿ç•™2ä½å°æ•°å¹¶ç¿»è¯‘
                            metrics_peak = {k: f"{v:.2f}" for k, v in peak_metrics.items()}
                            metrics_peak_cn = {
                                'ROC AUC': 'ROC AUC',
                                'PR AUC': 'PR AUC',
                                'Precision': 'ç²¾ç¡®ç‡',
                                'Recall': 'å¬å›ç‡',
                                'MCC': 'MCC'
                            }
                            metrics_peak_cn_translated = {metrics_peak_cn[k]: v for k, v in metrics_peak.items()}
                            st.table(pd.DataFrame(metrics_peak_cn_translated, index=[0]))
                        with c2:
                            st.markdown("#### ä½ç‚¹è¯†åˆ«")
                            st.write(f"**æœ€ä½³å¾—åˆ†ï¼š** {trough_best_score:.2f}")
                            st.write("**è¯„ä¼°æŒ‡æ ‡ï¼š**")
                            # å°†æŒ‡æ ‡ä¿ç•™2ä½å°æ•°å¹¶ç¿»è¯‘
                            metrics_trough = {k: f"{v:.2f}" for k, v in trough_metrics.items()}
                            metrics_trough_cn = {
                                'ROC AUC': 'ROC AUC',
                                'PR AUC': 'PR AUC',
                                'Precision': 'ç²¾ç¡®ç‡',
                                'Recall': 'å¬å›ç‡',
                                'MCC': 'MCC'
                            }
                            metrics_trough_cn_translated = {metrics_trough_cn[k]: v for k, v in metrics_trough.items()}
                            st.table(pd.DataFrame(metrics_trough_cn_translated, index=[0]))
                        
                    except Exception as e:
                        st.error(f"æ¨¡å‹è®­ç»ƒå¤±è´¥ï¼š{e}")
                        st.stop()
                
                # ========== ç¼“å­˜æ¨¡å‹åˆ° session_state ==========
                try:
                    st.session_state['peak_model'] = peak_model
                    st.session_state['peak_scaler'] = peak_scaler
                    st.session_state['peak_selector'] = peak_selector
                    st.session_state['all_features_peak'] = all_features_peak
                    st.session_state['peak_threshold'] = peak_threshold
                    
                    st.session_state['trough_model'] = trough_model
                    st.session_state['trough_scaler'] = trough_scaler
                    st.session_state['trough_selector'] = trough_selector
                    st.session_state['all_features_trough'] = all_features_trough
                    st.session_state['trough_threshold'] = trough_threshold
                except Exception as e:
                    st.error(f"ç¼“å­˜æ¨¡å‹å¤±è´¥ï¼š{e}")
                    st.stop()



##############################################################################
#                       éªŒè¯éƒ¨åˆ†å‡½æ•°
##############################################################################
def predict_section():
    st.header("æ¨¡å‹éªŒè¯æµç¨‹")
    st.markdown("""
    **åœ¨æ­¤éƒ¨åˆ†å¯å®Œæˆï¼š**  
    1. **è¯»å–æ–°æ•°æ®å¹¶è¿›è¡Œç›¸åŒçš„é¢„å¤„ç†**  
    2. **è°ƒç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œé«˜ä½ç‚¹éªŒè¯**   
    3. **å¯è§†åŒ–éªŒè¯ç»“æœ** 
    """)

    with st.expander("ğŸ”§ éªŒè¯æ•°æ®è®¾ç½®", expanded=True):
        with st.form("predict_form"):
            # éªŒè¯åŒºé—´
            st.markdown("ğŸ“… **éªŒè¯åŒºé—´**")
            col_pred_date1, col_pred_date2 = st.columns(2)
            with col_pred_date1:
                start_new_date = st.date_input(
                    "éªŒè¯å¼€å§‹æ—¥æœŸ", 
                    datetime.strptime("2021-01-01", "%Y-%m-%d"), 
                    key='pred_start_date',
                    help="é€‰æ‹©éªŒè¯æ•°æ®çš„å¼€å§‹æ—¥æœŸã€‚"
                )
            with col_pred_date2:
                end_new_date = st.date_input(
                    "éªŒè¯ç»“æŸæ—¥æœŸ", 
                    datetime.today(), 
                    key='pred_end_date',
                    help="é€‰æ‹©éªŒè¯æ•°æ®çš„ç»“æŸæ—¥æœŸã€‚"
                )

            # å…¶ä»–éªŒè¯å‚æ•°
            st.markdown("âš™ï¸ **å…¶ä»–éªŒè¯å‚æ•°**")
            # æ·»åŠ æœ€å°éªŒè¯é—´éš”å¤©æ•°
            min_days_between_predictions = st.number_input(
                "7ï¸âƒ£ æœ€å°éªŒè¯é—´éš”å¤©æ•°",  # ä¿®æ”¹ç¼–å·
                min_value=1,
                max_value=365,
                value=20,
                help="åœ¨æŒ‡å®šçš„å¤©æ•°å†…ä¸å…è®¸é‡å¤éªŒè¯ï¼ˆå³ç›¸åŒç±»åˆ«çš„é¢„æµ‹ç‚¹ä¹‹é—´è‡³å°‘ç›¸éš”å¤šå°‘å¤©ï¼‰ã€‚"
            )

            submit_predict = st.form_submit_button("æäº¤éªŒè¯å‚æ•°")
        
        if submit_predict:
            st.session_state['predict_params'] = {
                'start_new_date': start_new_date,
                'end_new_date': end_new_date,
                'min_days_between_predictions': min_days_between_predictions
            }
            st.success("éªŒè¯å‚æ•°å·²æäº¤ï¼Œè¯·ç‚¹å‡»ä¸‹æ–¹ã€è°ƒç”¨æ¨¡å‹è¿›è¡ŒéªŒè¯ã€æŒ‰é’®å¼€å§‹éªŒè¯ã€‚")

    st.markdown("---")
    st.subheader("ğŸ”„ å¼€å§‹éªŒè¯")
    st.markdown("è¯·ç‚¹å‡»ä¸‹æ–¹æŒ‰é’®è¿›è¡ŒéªŒè¯ï¼Œå¹¶å¯æŸ¥çœ‹éªŒè¯åé«˜ä½ç‚¹å¯è§†åŒ–ç»“æœã€‚")

    if st.button("è°ƒç”¨æ¨¡å‹è¿›è¡ŒéªŒè¯"):
        if not all([
            'peak_model' in st.session_state, 
            'trough_model' in st.session_state,
            'predict_params' in st.session_state,
            'train_params' in st.session_state  # ç¡®ä¿è®­ç»ƒå‚æ•°å­˜åœ¨ä»¥è·å–ä»£ç ç±»å‹
        ]):
            st.error("è¯·å…ˆåœ¨å·¦ä¾§ã€æ¨¡å‹è®­ç»ƒã€éƒ¨åˆ†å®Œæˆæ¨¡å‹è®­ç»ƒåå†è¿›è¡ŒéªŒè¯ã€‚")
        else:
            params = st.session_state['predict_params']
            start_new_date = params['start_new_date']
            end_new_date = params['end_new_date']
            min_days_between_predictions = params['min_days_between_predictions']

            # å‚æ•°éªŒè¯
            if start_new_date > end_new_date:
                st.error("éªŒè¯å¼€å§‹æ—¥æœŸä¸èƒ½æ™šäºéªŒè¯ç»“æŸæ—¥æœŸã€‚")
            else:
                with st.spinner("æ­£åœ¨è¯»å–éªŒè¯æ•°æ®å¹¶è¿›è¡Œé¢„å¤„ç†..."):
                    try:
                        # è·å–è®­ç»ƒå‚æ•°ä»¥è·å– symbol_type å’Œ symbol_code
                        train_params = st.session_state['train_params']
                        symbol_type = train_params['symbol_type']
                        symbol_code = train_params['symbol_code']

                        # æ˜¾ç¤ºå½“å‰ä½¿ç”¨çš„ symbol_type å’Œ symbol_code ä»¥ç¡®è®¤
                        st.markdown(f"**ä½¿ç”¨çš„ä»£ç ç±»å‹ï¼š** {'è‚¡ç¥¨' if symbol_type == 'stock' else 'æŒ‡æ•°'}")
                        st.markdown(f"**ä½¿ç”¨çš„ä»£ç ï¼š** {symbol_code}")

                        # è¯»å–æ•°æ®
                        data = read_day_from_tushare(symbol_code, symbol_type=symbol_type)
                        if data.empty:
                            st.error("é€šè¿‡ Tushare è·å–çš„æ•°æ®ä¸ºç©ºï¼Œè¯·æ£€æŸ¥ä»£ç ç±»å‹å’Œä»£ç ã€‚")
                            st.stop()
                        
                        # æˆªå–éªŒè¯åŒºé—´
                        new_df = select_time(data, start_new_date.strftime("%Y%m%d"), end_new_date.strftime("%Y%m%d"))
                        if new_df.empty:
                            st.error("éªŒè¯é›†ä¸ºç©ºï¼Œè¯·æ£€æŸ¥æ—¥æœŸèŒƒå›´å’Œä»£ç ã€‚")
                            st.stop()
                        
                        # è°ƒç”¨éªŒè¯
                        result = predict_new_data(
                            new_df,
                            _peak_model=st.session_state['peak_model'], 
                            _peak_scaler=st.session_state['peak_scaler'], 
                            _peak_selector=st.session_state['peak_selector'], 
                            all_features_peak=st.session_state['all_features_peak'], 
                            peak_threshold=st.session_state['peak_threshold'],
                            _trough_model=st.session_state['trough_model'], 
                            _trough_scaler=st.session_state['trough_scaler'], 
                            _trough_selector=st.session_state['trough_selector'], 
                            all_features_trough=st.session_state['all_features_trough'], 
                            trough_threshold=st.session_state['trough_threshold'],
                            N=st.session_state['train_params']['N'], 
                            mixture_depth=st.session_state['train_params']['mixture_depth'], 
                            min_days_between_predictions=min_days_between_predictions
                        )
                        
                        st.success("éªŒè¯å®Œæˆï¼")
                        
                        # ========== æ˜¾ç¤ºéªŒè¯ç»“æœçš„å›¾è¡¨ ==========
                        st.subheader("ğŸ“Š éªŒè¯ç»“æœå¯è§†åŒ–")
                        peaks_pred = result[result['Peak_Prediction'] == 1]
                        troughs_pred = result[result['Trough_Prediction'] == 1]
                        symbol_display = symbol_code
                        fig_pred = plot_candlestick_plotly(
                            result, symbol_display, start_new_date.strftime("%Y%m%d"), end_new_date.strftime("%Y%m%d"),
                            peaks=peaks_pred, troughs=troughs_pred, prediction=True, 
                            selected_classifiers=[st.session_state['train_params']['classifier_name']]
                        )
                        st.plotly_chart(fig_pred, use_container_width=True)

                        # ========== æ˜¾ç¤ºéªŒè¯ç»“æœçš„æ•°æ®è¡¨ ==========
                        st.subheader("ğŸ“„ éªŒè¯ç»“æœï¼ˆä»…æ˜¾ç¤ºéªŒè¯åˆ°çš„é«˜ä½ç‚¹ï¼‰")
                        filtered_result = result[
                            (result['Peak_Prediction'] == 1) | (result['Trough_Prediction'] == 1)
                        ]
                        
                        if filtered_result.empty:
                            st.info("æ²¡æœ‰éªŒè¯åˆ°é«˜ç‚¹æˆ–ä½ç‚¹ã€‚")
                        else:
                            result_table = filtered_result[['TradeDate', 'Peak_Prediction', 'Peak_Probability',
                                                            'Trough_Prediction', 'Trough_Probability']].copy()
                            result_table = result_table.rename(columns={
                                'Peak_Prediction': 'é«˜ç‚¹',
                                'Peak_Probability': 'é«˜ç‚¹æ¦‚ç‡',
                                'Trough_Prediction': 'ä½ç‚¹',
                                'Trough_Probability': 'ä½ç‚¹æ¦‚ç‡'
                            })
                            # ä¿ç•™2ä½å°æ•°
                            result_table['é«˜ç‚¹æ¦‚ç‡'] = result_table['é«˜ç‚¹æ¦‚ç‡'].astype(float).round(2)
                            result_table['ä½ç‚¹æ¦‚ç‡'] = result_table['ä½ç‚¹æ¦‚ç‡'].astype(float).round(2)
                            
                            AgGrid(result_table, fit_columns_on_grid_load=True, height=300, theme='streamlit')
                            
                            # æ·»åŠ ä¸‹è½½æŒ‰é’®
                            st.markdown(download_link(result_table, 'validation_results.csv', 'ğŸ“¥ ä¸‹è½½éªŒè¯ç»“æœ'), unsafe_allow_html=True)
                        
                    except AssertionError as ae:
                        st.error(f"éªŒè¯æ•°æ®å¤„ç†æ–­è¨€å¤±è´¥ï¼š{ae}")
                        st.stop()
                    except Exception as e:
                        st.error(f"éªŒè¯å¤±è´¥ï¼š{e}")
                        st.stop()

##############################################################################
#                       å…¶ä»–å¿…è¦å‡½æ•°å’Œå¯¼å…¥
##############################################################################
# è¯·ç¡®ä¿æ‚¨å·²ç»åœ¨ function.py ä¸­å®šä¹‰äº†æ‰€æœ‰å¿…è¦çš„è®¡ç®—å‡½æ•°ï¼Œå¦‚ compute_RSIã€compute_MACD ç­‰ã€‚
# åŒæ—¶ï¼Œè¯·ç¡®ä¿ models.py ä¸­å®šä¹‰äº†æ‰€æœ‰è‡ªå®šä¹‰æ¨¡å‹ç±»ã€‚

##############################################################################
#                       è¿è¡Œåº”ç”¨
##############################################################################
if __name__ == "__main__":
    multiprocessing.freeze_support()
    main()
